# ğŸš´â€â™‚ï¸ BikeStores Data Engineering Project â€” PySpark + dbt + Snowflake

This project implements a modular and scalable data platform that:

- Ingests CSV files from the BikeStores dataset into Snowflake using PySpark
- Transforms data into business-friendly models using dbt's bronze â†’ silver â†’ gold layering
---

## ğŸ“ Project Structure

```
bikestores-project/
â”‚
â”œâ”€â”€ python/ # Data ingestion pipeline using PySpark
â”‚ â”œâ”€â”€ config/
â”‚ â”‚ â””â”€â”€ pipeline_config.py # Configuration for staging and Snowflake
â”‚ â”‚
â”‚ â”œâ”€â”€ staging/
â”‚ â”‚ â””â”€â”€ csv_files/ # Raw BikeStores CSV files
â”‚ â”‚
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ process_pipeline.py # Main ETL pipeline: load CSV to Snowflake
â”‚ â”‚ â”œâ”€â”€ conn.py # Snowflake connection test and setup
â”‚ â”‚ â””â”€â”€ utils.py # Helper functions: config parsing, data load
â”‚ â”‚
â”‚ â”œâ”€â”€ requirements.txt # Python dependencies
â”‚ â”œâ”€â”€ LICENSE # Project license (MIT)
â”‚ â””â”€â”€ README.md # This file
â”‚
â”œâ”€â”€ dbt/ # dbt project for Snowflake modeling
â”‚ â”œâ”€â”€ macros/
â”‚ â”‚ â””â”€â”€ generate_schema_name.sql
â”‚ â”‚
â”‚ â””â”€â”€ models/
â”‚ â”œâ”€â”€ bronze/ # Raw tables from Snowflake ingestion
â”‚ â”œâ”€â”€ silver/ # Cleaned and joined datasets
â”‚ â””â”€â”€ gold/ # Business-ready fact/dimension models
```
---

## ğŸ”§ Features

### âœ… PySpark Pipeline (under python/)
- ğŸ” Secure Snowflake auth using PKCS8 private key
- ğŸ“¦ Modular ETL logic using config-driven setup
- ğŸ§® Automatic table inference from filenames (e.g., prod_orders.csv â†’ orders)
- â„ï¸ Optimized integration with Snowflake Spark connector
  * Uses `net.snowflake.spark.snowflake` Spark connector.
  * Supports overwrite mode for clean data reloads.

### âœ… dbt Transformations (under dbt/)
- ğŸ¥‰ Bronze layer for raw ingestion copies
- ğŸ¥ˆ Silver layer for cleaned and modeled views
- ğŸ¥‡ Gold layer for metrics, summaries, and business logic
- ğŸ§  Includes custom macro for schema naming

---

## ğŸ§¹ Dataset

Download the BikeStores sample dataset or generate your own CSVs.
Place the data files inside:

```
staging/csv_files/
```

Each file should follow the naming convention:

```
<environment>_<table>.csv
# Example: production_brands.csv â†’ loads into "brands" table
```

---

## âš™ï¸ Configuration (`pipeline_config.py`)

Example structure:

```ini
[STAGING]
input_dataset_path = <path-to-dataset-dir>

[SNOWFLAKE]
private_key_file_path = <path-to-snowflake-key-pkcs8.pem>
raw_database = SQLSERVER_DB
raw_schema = BRONZE_SCH
warehouse = ETL_WH
account = "<your_account_locator>",
role = DATA_LOADER
user = SNOWFLAKE_ETL
```

---

## ğŸ”— Installation
1. Clone the repository:
   ```sh
   git clone https://github.com/neeluvickey/bikestores-project.git
   cd bikestores-project
   ```
2. Create a virtual environment and activate it:
   ```sh
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. Install dependencies:
   ```sh
   pip install -r requirements.txt
   ```

---

## ğŸš€ How to Run

1. **Run the Pipeline**:

   ```bash
   python src/process_pipeline.py
   ```

   This will:

   * Read all CSVs under `staging/csv_files/`
   * Infer schema and load into appropriate Snowflake tables

---

## ğŸ§  Key Functions

### `parse_config(config_path)`

Loads the Python-based config dictionary.

### `data_load(spark, file_path, config)`

Reads a single CSV and writes it to Snowflake using the Spark Snowflake connector.

---

## ğŸ“Œ Notes

* Ensure your private key is in **PKCS8 format** for Snowflake Spark connector compatibility.
* Pipeline currently supports `.csv` files only (JSON extensions could be added).
* Snowflake connector must be properly referenced in your Spark setup.

---

## ğŸ“ License

This project is licensed under the [MIT License](LICENSE).
---
